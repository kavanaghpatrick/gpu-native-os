# GPU-Native OS

A single-threadgroup operating system running entirely on the GPU via Metal compute shaders.

## Overview

This project explores a radical approach to application architecture: instead of the CPU orchestrating rendering with GPU assistance, **the GPU runs the entire application** — input handling, state updates, layout, and rendering — in a single compute kernel per frame.

All 1024 threads participate in every phase. The CPU's role is minimal: forward input events and submit command buffers.

## Architecture

### High-Level Design

```
+-------------------------------------------------------------------+
|                     CPU (Rust Host) - MINIMAL                     |
|  +-------------+  +-------------+  +-------------------------+    |
|  | Buffer Setup|  |Command Queue|  | Async Notification      |    |
|  | (one-time)  |  | Dispatch    |  | (SharedEvent callback)  |    |
|  +-------------+  +-------------+  +-------------------------+    |
+-----------------------------------+-------------------------------+
                                    | Metal API (Non-blocking)
+-----------------------------------v-------------------------------+
|                      GPU (Metal Compute)                          |
|  +-------------+  +-------------+  +-------------------------+    |
|  | Tokenization|  | Fuzzy Search|  | Text Generation         |    |
|  | (GPU parse) |  | 3M parallel |  | (vertex generation)     |    |
|  |             |  | threads     |  |                         |    |
|  +-------------+  +-------------+  +-------------------------+    |
|                                                                   |
|  +---------------------------------------------------------------+|
|  |  GPU-Resident Data (Zero-Copy via mmap)                       ||
|  |  - Filesystem Index: mmap -> Metal buffer (never copied)      ||
|  |  - Query Strings: raw bytes -> GPU tokenizes                  ||
|  |  - Search Results: GPU writes -> GPU renders                  ||
|  +---------------------------------------------------------------+|
|  Unified Memory (MTLStorageModeShared) - Zero-copy on Apple Si    |
+-------------------------------------------------------------------+
```

### Why GPU-Native?

**Apple Silicon unified memory** eliminates CPU-GPU copies. App state lives in shared buffers accessible to both. The GPU reads input, processes state, and renders — all without round-trips.

**Single threadgroup** means all 1024 threads share 32KB of fast threadgroup memory. They can synchronize with barriers and use SIMD operations for parallel reductions.

**Branchless SIMD** keeps all threads executing the same instructions, avoiding warp divergence that kills GPU performance.

### Frame Execution

Each frame, `GpuRuntime` executes:

```
1. Update timing (delta_time, frame_count)
2. Process input events -> forward to app
3. App updates params buffer
4. COMPUTE PASS: dispatch app kernel (1024 threads)
5. RENDER PASS: draw vertices generated by compute
6. Present drawable
```

---

## GPU vs CPU: What Runs Where

Understanding what actually runs on the GPU vs CPU is crucial for this architecture.

### Before vs After: CPU Work Eliminated

| Operation | Before (CPU) | After (GPU) | Improvement |
|-----------|--------------|-------------|-------------|
| Query tokenization | `split_whitespace()` | GPU kernel (1 thread/char) | **Moved to GPU** |
| Index loading | `fs::read()` + copy | mmap + zero-copy buffer | **Zero copies** |
| Search synchronization | `wait_until_completed()` | SharedEvent callback | **Non-blocking** |
| Result accumulation | CPU loop | Atomic GPU writes | **Parallel** |

### Current Work Division

| GPU Work (Metal Compute) | CPU Work (Rust) |
|--------------------------|-----------------|
| Query tokenization (1 thread per character) | Raw query bytes memcpy (ONE copy) |
| Fuzzy string matching (3M parallel threads) | Command buffer creation |
| Case-insensitive comparison | Metal API dispatch calls |
| Filename extraction (find last `/`) | Async completion callback |
| Path depth calculation | |
| Score calculation | |
| Atomic result slot allocation | |
| Sorting of results | |
| Text character generation | |

### Memory Model

```rust
desc.set_storage_mode(MTLStorageMode::Shared);
```

On Apple Silicon (M1/M2/M3/M4):
- CPU and GPU share the same physical memory
- No DMA transfers needed
- `buffer.contents()` returns a pointer both can use
- Changes are immediately visible (with proper synchronization)

---

## Foundation Components

### 1. Zero-Copy mmap Buffer (`mmap_buffer.rs`)

**Issue #82**: File data goes directly to GPU without any CPU copies.

```
Traditional:  File -> read() -> CPU buffer -> copy -> GPU buffer
Zero-Copy:    File -> mmap() -> newBufferWithBytesNoCopy() -> GPU buffer
                                (same physical memory!)
```

#### Implementation

```rust
pub struct MmapBuffer {
    mmap_ptr: *mut c_void,    // mmap'd file region
    mmap_len: usize,          // Page-aligned size
    file_size: usize,         // Original file size
    _file: File,              // Keeps file open
    buffer: Buffer,           // Metal buffer (same memory!)
}

impl MmapBuffer {
    pub fn from_file(device: &Device, path: &Path) -> Result<Self, MmapError> {
        // 1. Open file
        let file = File::open(path)?;
        let file_size = file.metadata()?.len() as usize;

        // 2. mmap the file (page-aligned)
        let aligned_size = align_to_page(file_size);
        let mmap_ptr = unsafe {
            libc::mmap(
                std::ptr::null_mut(),
                aligned_size,
                libc::PROT_READ,
                libc::MAP_PRIVATE,
                file.as_raw_fd(),
                0,
            )
        };

        // 3. Create Metal buffer pointing to SAME MEMORY
        let buffer = device.new_buffer_with_bytes_no_copy(
            mmap_ptr,
            aligned_size as u64,
            MTLResourceOptions::StorageModeShared,
            None,  // No deallocation callback - we own the mmap
        );

        Ok(Self { mmap_ptr, mmap_len, file_size, _file: file, buffer })
    }
}
```

#### GPU vs CPU Comparison

| Metric | Traditional (read + copy) | Zero-Copy (mmap) |
|--------|---------------------------|------------------|
| Memory copies | 2 (disk->CPU, CPU->GPU) | 0 |
| Memory usage | 2x file size | 1x file size |
| Load time (10MB) | ~15ms | ~0.5ms |
| CPU involvement | High | None after setup |

---

### 2. GPU-Resident Filesystem Index (`gpu_index.rs`)

**Issue #77**: Filesystem index lives permanently in GPU memory. CPU scans ONCE, GPU owns data forever.

```
Traditional:  CPU scans -> copies paths -> GPU searches -> repeat every search
GPU-Resident: CPU scans ONCE -> mmap index -> GPU owns data forever
```

#### Data Structures

```rust
/// 256-byte cache-aligned path entry (GPU-friendly)
#[repr(C)]
pub struct GpuPathEntry {
    pub path: [u8; 224],      // Fixed-width path (null-padded)
    pub path_len: u16,        // Actual length
    pub flags: u16,           // is_dir, is_hidden, etc.
    pub parent_idx: u32,      // Parent directory index
    pub size: u64,            // File size in bytes
    pub mtime: u64,           // Modification time
    pub _reserved: [u8; 8],   // Padding to 256 bytes
}

/// Index file format
+------------------+
| Header (4096B)   |  <- Page-aligned for mmap
|  - magic: GPUI   |
|  - version: 1    |
|  - entry_count   |
+------------------+
| Entry 0 (256B)   |  <- Cache-aligned
+------------------+
| Entry 1 (256B)   |
+------------------+
| ...              |
+------------------+
```

#### Usage

```rust
// Build index once (slow, ~30s for 3M files)
GpuResidentIndex::build_and_save("/", "index.bin", None)?;

// Load index (instant via mmap - same memory as file!)
let index = GpuResidentIndex::load(&device, "index.bin")?;

// GPU now owns the data - search without ANY copies
encoder.set_buffer(0, Some(index.entries_buffer()), 0);
```

#### GPU vs CPU Comparison

| Metric | Traditional Index | GPU-Resident Index |
|--------|-------------------|-------------------|
| Load time (1M entries) | ~500ms | <1ms |
| Memory copies on load | 1 (file->buffer) | 0 (mmap) |
| Index ownership | CPU | GPU |
| Per-search copies | Path data copied | Zero |

---

### 3. GPU String Processing (`filesystem.rs`)

**Issue #79**: Query tokenization runs on GPU, not CPU.

```
Traditional:  CPU splits query -> copies words -> GPU searches
GPU-Native:   CPU copies raw bytes (1 memcpy) -> GPU tokenizes + searches
```

#### GPU Tokenization Kernel

```metal
kernel void tokenize_query_kernel(
    device const char* raw_query [[buffer(0)]],
    device TokenizeResult* result [[buffer(1)]],
    constant uint& query_len [[buffer(2)]],
    uint tid [[thread_position_in_grid]]
) {
    // One thread per character
    if (tid >= query_len) return;

    char c = raw_query[tid];
    bool is_space = (c == ' ' || c == '\t' || c == '\n');
    bool prev_space = (tid == 0) || is_space_char(raw_query[tid-1]);

    // Detect word starts (transition from space to non-space)
    if (!is_space && prev_space) {
        // Atomically allocate word slot
        uint slot = atomic_fetch_add_explicit(&result->word_count, 1, memory_order_relaxed);
        if (slot < MAX_QUERY_WORDS) {
            result->word_starts[slot] = tid;
            // Find word end...
        }
    }
}
```

#### CPU Work: ONE memcpy

```rust
pub fn search_and_render(&mut self, query: &str, ...) -> usize {
    let bytes = query.as_bytes();

    // THE ONLY CPU WORK: Copy raw query bytes to GPU buffer
    unsafe {
        let query_ptr = self.raw_query_buffer.contents() as *mut u8;
        std::ptr::copy_nonoverlapping(bytes.as_ptr(), query_ptr, bytes.len());
    }

    // Pass 0: GPU Tokenization
    // Pass 1: GPU Search
    // Pass 2: GPU Sort
    // Pass 3: GPU Text Generation
    // ... all on GPU, CPU just dispatches
}
```

#### GPU vs CPU Comparison

| Operation | Before (CPU) | After (GPU) |
|-----------|--------------|-------------|
| Tokenization | `query.split_whitespace()` | GPU kernel |
| Word extraction | CPU string ops | Parallel atomic slots |
| Case conversion | `to_lowercase()` | GPU inline |
| CPU work per search | Parse + copy words | 1 memcpy of raw bytes |

---

### 4. Async GPU Pipeline (`filesystem.rs`)

**Issue #76**: Non-blocking search via SharedEvent. CPU never waits for GPU.

```
Traditional:  dispatch -> wait_until_completed() -> BLOCKED -> read results
Async:        dispatch -> encode_signal_event() -> return immediately
              ... later: SharedEventListener callback fires
```

#### SearchHandle API

```rust
pub struct SearchHandle {
    shared_event: SharedEvent,
    signal_value: u64,
    results_buffer: Buffer,
    result_count_buffer: Buffer,
    max_results: usize,
    callback_fired: Arc<AtomicBool>,
}

impl SearchHandle {
    /// Check if GPU has finished (non-blocking)
    pub fn is_complete(&self) -> bool {
        self.callback_fired.load(Ordering::Acquire)
    }

    /// Get results if complete
    pub fn try_get_results(&self) -> Option<Vec<SearchResult>> {
        if self.is_complete() {
            Some(self.read_results())
        } else {
            None
        }
    }

    /// Block until complete (fallback)
    pub fn wait(&self) -> Vec<SearchResult> {
        while !self.is_complete() {
            std::thread::sleep(Duration::from_micros(100));
        }
        self.read_results()
    }
}
```

#### Usage Pattern

```rust
// Start search (returns immediately!)
let handle = search.search_async("my query", 100);

// Do other work while GPU searches...
update_ui();
process_input();

// Check completion (non-blocking)
if handle.is_complete() {
    let results = handle.try_get_results().unwrap();
    display_results(results);
}
```

#### GPU vs CPU Comparison

| Metric | Blocking | Async (SharedEvent) |
|--------|----------|---------------------|
| CPU blocked during search | 100% | 0% |
| `wait_until_completed()` calls | 1 per search | 0 |
| UI responsiveness | Frozen | Smooth |
| Completion notification | Polling | Callback |

---

## Major Components

### GPU Filesystem Search (`filesystem.rs`)

The filesystem module implements GPU-accelerated fuzzy path search capable of searching 3+ million paths in real-time.

#### Complete Search Pipeline

```
Pass 0: tokenize_query_kernel
  - 256 threads (1 per character max)
  - Tokenizes raw query bytes into words
  - Atomic word slot allocation

Pass 1: fuzzy_search_kernel
  - 11,719 threadgroups x 256 threads = 3M parallel searches
  - Each thread scores one path against query words
  - Atomic counter allocates result slots

Pass 2: sort_results_kernel
  - Single thread (serial sort)
  - Insertion sort limited to 100 items

Pass 3: generate_results_text_kernel
  - 1 threadgroup for up to 256 results
  - Generates TextChar array for direct rendering
```

#### Streaming Search Architecture

```
+-------------------------------------------------------------------+
|                     Streaming Search Pipeline                      |
+-------------------------------------------------------------------+

Disk Index -> [50K chunk] -> GPU Search -> Merge -> [50K chunk] -> GPU -> ...
                  |                          ^
             12MB buffer              Top results accumulated
```

#### Thread Parallelism

For 3 million paths:
```
paths = 3,000,000
threads_per_group = 256
thread_groups = ceil(3,000,000 / 256) = 11,719
total_threads = 3,000,064 (one thread per path)
```

Each GPU thread independently:
1. Loads its assigned path from `paths + (gid * 256)`
2. Checks all query words against the filename
3. Calculates fuzzy match score with bonuses/penalties
4. Atomically appends to results if matched

---

### Text Rendering (`text_render.rs`)

The bitmap font text renderer follows a **CPU-prepares, GPU-draws** pattern.

#### Architecture

```
+------------------+     +--------------------+     +------------------+
|     CPU Work     |     |    GPU Memory      |     |    GPU Work      |
+------------------+     +--------------------+     +------------------+
| Parse text       |---->| chars_buffer       |---->| Vertex shader:   |
| Build TextChar[] |     | (TextChar array)   |     |  - Generate quads|
| Upload buffer    |     |                    |     |  - Compute UVs   |
|                  |     | uniforms_buffer    |     |  - Transform pos |
| Font generation: |     | (screen size,scale)|     |                  |
| - 8x8 bitmaps    |---->|                    |---->| Fragment shader: |
| - Texture upload |     | font_texture       |     |  - Sample texture|
|                  |     | (128x48 R8)        |     |  - Alpha test    |
+------------------+     +--------------------+     +------------------+
```

#### Key Insight: Procedural Vertices

The CPU does NOT create vertex data. Instead:

```rust
struct TextChar {
    x: f32,         // Screen position X
    y: f32,         // Screen position Y
    char_code: u32, // ASCII code
    color: u32,     // RGBA packed color
}
```

The GPU vertex shader generates 6 vertices per character procedurally:
```metal
uint char_idx = vid / 6;   // Which character
uint vert_idx = vid % 6;   // Which vertex of the 6-vertex quad
```

This means a single draw call renders ALL text:
```rust
encoder.draw_primitives(MTLPrimitiveType::Triangle, 0, (char_count * 6) as u64);
```

---

## GpuApp Framework

The `GpuApp` trait provides a standardized way to build GPU-native applications:

```rust
pub trait GpuApp {
    fn name(&self) -> &str;
    fn compute_pipeline(&self) -> &ComputePipelineState;
    fn render_pipeline(&self) -> &RenderPipelineState;
    fn vertices_buffer(&self) -> &Buffer;
    fn vertex_count(&self) -> usize;
    fn app_buffers(&self) -> Vec<&Buffer>;
    fn params_buffer(&self) -> &Buffer;
    fn update_params(&mut self, frame_state: &FrameState, delta_time: f32);
    fn handle_input(&mut self, event: &InputEvent);

    // Optional text rendering
    fn uses_text_rendering(&self) -> bool { false }
    fn render_text(&mut self, text_renderer: &mut TextRenderer) {}
}
```

### Buffer Slot Convention

All apps follow standardized buffer binding:

| Slot | Buffer | Purpose |
|------|--------|---------|
| 0 | FrameState | OS-provided: cursor, time, frame number |
| 1 | InputQueue | OS-provided: keyboard/mouse events |
| 2 | AppParams | App-specific per-frame parameters |
| 3+ | App buffers | App-specific state |

### Pipeline Modes

Apps can choose their latency/throughput tradeoff:

```rust
fn pipeline_mode(&self) -> PipelineMode {
    PipelineMode::HighThroughput  // Simulations: allow frame overlap
    // or
    PipelineMode::LowLatency      // Text editors: minimize input delay
}
```

| Mode | Behavior | Best For |
|------|----------|----------|
| `LowLatency` | Wait for previous frame | Text editors, interactive UI |
| `HighThroughput` | Allow frames to overlap | Simulations, animations, games |

---

## Project Structure

```
src/
├── lib.rs                    # Library entry point
├── gpu_os/
│   ├── mod.rs                # Module exports
│   │
│   │── Foundation (NEW) ────────────────────────────────────────
│   ├── mmap_buffer.rs        # #82 - Zero-copy file-to-GPU access
│   ├── gpu_index.rs          # #77 - GPU-Resident Filesystem Index
│   ├── metal_types.rs        # Metal-safe struct definitions
│   │
│   │── Core Framework ──────────────────────────────────────────
│   ├── app.rs                # GpuApp trait and GpuRuntime
│   ├── kernel.rs             # Unified Worker Model (compute kernel)
│   ├── memory.rs             # Memory Architecture (GPU buffers)
│   ├── input.rs              # Input Pipeline (HID to GPU)
│   ├── render.rs             # Hybrid Rendering (compute + fragment)
│   ├── vsync.rs              # VSync Execution (frame timing)
│   │
│   │── Text Rendering ──────────────────────────────────────────
│   ├── text_render.rs        # Bitmap font renderer (production)
│   ├── text.rs               # MSDF text (experimental)
│   │
│   │── Layout & Widgets ────────────────────────────────────────
│   ├── layout.rs             # Constraint-based layout engine
│   ├── widget.rs             # Widget system (compressed state)
│   │
│   │── GPU Applications ────────────────────────────────────────
│   ├── filesystem.rs         # GPU filesystem search (3M+ paths)
│   │                         #   - #76: Async search (SharedEvent)
│   │                         #   - #79: GPU tokenization
│   ├── content_search.rs     # GPU content search
│   ├── duplicate_finder.rs   # GPU duplicate file detection
│   ├── document/             # GPU document viewer
│   ├── vector/               # GPU vector rasterizer
│   │
│   │── Demos ───────────────────────────────────────────────────
│   ├── game_of_life.rs       # Conway's Game of Life
│   ├── particles.rs          # Particle system
│   ├── boids.rs              # Flocking simulation
│   ├── mandelbrot.rs         # Fractal viewer
│   ├── metaballs.rs          # Organic blob rendering
│   └── waves.rs              # Wave simulation

examples/
├── filesystem_browser.rs     # Interactive file browser (main demo)
├── document_viewer.rs        # Document rendering demo
├── benchmark_*.rs            # Performance benchmarks
└── *.rs                      # Various demos

tests/
├── test_issue_76_async_search.rs   # Async GPU pipeline tests
├── test_issue_77_gpu_index.rs      # GPU-resident index tests
├── test_issue_79_gpu_string.rs     # GPU string processing tests
├── test_issue_82_mmap_buffer.rs    # Zero-copy mmap tests
├── test_streaming_search.rs        # Streaming search tests
└── test_*.rs                       # Other integration tests
```

---

## Performance

### Benchmarks (Apple M4 Pro)

The GPU-Native OS thesis: **"1024 threads doing UI logic together beats 1 CPU thread."**

#### Key Finding: GPU Wins on Architecture, Not Raw Speed

| Scenario | Traditional (3 dispatches) | GPU-Native (1 dispatch) | Speedup |
|----------|---------------------------|------------------------|---------|
| 10 widgets | 313 us | 99 us | **3.2x GPU** |
| 50 widgets | 270 us | 99 us | **2.7x GPU** |
| 200 widgets | 284 us | 89 us | **3.2x GPU** |
| 500 widgets | 319 us | 121 us | **2.6x GPU** |
| 1000 widgets | 348 us | 110 us | **3.2x GPU** |

The win comes from **eliminating CPU-GPU sync points**, not raw compute speed.

#### Frame Pipelining: 6.75x Throughput

| Mode | 1000 Frames | FPS |
|------|-------------|-----|
| Serial (wait each frame) | 104.6 ms | 9,562 |
| Pipelined (overlap frames) | 15.5 ms | 64,584 |
| **Speedup** | | **6.75x** |

#### Zero-Copy Performance (NEW)

| Operation | Traditional | Zero-Copy | Improvement |
|-----------|-------------|-----------|-------------|
| Load 10MB index | ~15ms | <1ms | **15x faster** |
| Memory usage | 2x file size | 1x file size | **50% less** |
| CPU copies | 2 per load | 0 | **Eliminated** |

#### Async Search Performance (NEW)

| Metric | Blocking | Async |
|--------|----------|-------|
| UI freeze during search | Yes | No |
| CPU utilization during search | 100% (waiting) | <5% |
| Search initiation | Sequential | Can overlap |

#### Filesystem Search Performance

| Metric | Value |
|--------|-------|
| Paths supported | 3,000,000+ (streaming) |
| Memory usage | ~24MB fixed (chunked) |
| Search latency | ~260ms per 50K chunk |
| Threads per search | 1 per path (3M threads) |
| Threadgroup size | 256 |
| CPU work per search | 1 memcpy (query bytes) |

### Run Benchmarks

```bash
# Automated benchmark suite
cargo run --release --example benchmark_auto -- --iterations 100

# Visual real-time comparison
cargo run --release --example benchmark_visual
```

---

## Demo Applications

### Main Applications

| Demo | Description | Command |
|------|-------------|---------|
| **Filesystem Browser** | GPU-accelerated file search (3M+ paths) | `cargo run --release --example filesystem_browser` |
| **Document Viewer** | GPU document rendering | `cargo run --release --example document_viewer` |
| **Text Editor** | Full text editor with GPU-rendered font | `cargo run --release --example text_editor` |

### Simulations

| Demo | Description | Command |
|------|-------------|---------|
| **Waves** | 256x256 wave simulation with ripple effects | `cargo run --release --example waves` |
| **Boids** | 1024-boid flocking (separation, alignment, cohesion) | `cargo run --release --example boids` |
| **Particles** | 10K+ particle physics with mouse interaction | `cargo run --release --example particles` |
| **Metaballs** | Organic blob animation with implicit surfaces | `cargo run --release --example metaballs` |
| **Game of Life** | Conway's cellular automaton (32x32 grid) | `cargo run --release --example game_of_life` |
| **Mandelbrot** | Fractal viewer with 10 preset locations | `cargo run --release --example mandelbrot` |

### Filesystem Browser Controls
- **Type** - Fuzzy search across all files
- **Up/Down** - Navigate results
- **Enter** - Open selected file
- **Escape** - Clear search

---

## Quick Start

```bash
# Build all examples
cargo build --release

# Run filesystem browser (main demo)
cargo run --release --example filesystem_browser

# Run other demos
cargo run --release --example waves
cargo run --release --example boids
cargo run --release --example mandelbrot

# Run tests
cargo test

# Run specific test suites
cargo test --test test_issue_76_async_search
cargo test --test test_issue_77_gpu_index
cargo test --test test_issue_79_gpu_string
cargo test --test test_issue_82_mmap_buffer
```

## Requirements

- macOS with Apple Silicon (M1/M2/M3/M4) or AMD GPU
- Rust 1.70+
- Xcode Command Line Tools (for Metal compiler)

## License

MIT

## Acknowledgments

Inspired by Casey Muratori's discussions on GPU-driven UI and Apple's TBDR (Tile-Based Deferred Rendering) architecture.
